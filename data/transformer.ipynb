{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb0087e6-3cf4-4185-ae00-a772bc5311a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20404 entries, 0 to 20403\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   song_id  20404 non-null  object\n",
      " 1   lyrics   19663 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 318.9+ KB\n",
      "                      song_id  \\\n",
      "0      3e9HZxeyfWwjeyPAMmWSSQ   \n",
      "1      5p7ujcrUXASCNwRaWNHR1C   \n",
      "2      2xLMifQCjDGFmkHkpNLD9h   \n",
      "4      1rqqCSm0Qe4I9rUvWncaom   \n",
      "5      0bYg9bo50gSsH3LtXe2SQn   \n",
      "...                       ...   \n",
      "20399  2pMAmZdHfQHyqJCXJbfhK3   \n",
      "20400  0IaMMHVbpJ0LrRAeigWOXr   \n",
      "20401  4nASzyRbzL5qZQuOPjQfsj   \n",
      "20402  2F4FNcz68howQWD4zaGJSi   \n",
      "20403  0TEQ2QmFXnHCgQvYuvsbp2   \n",
      "\n",
      "                                                  lyrics  \\\n",
      "0      ['[Verse 1]\\nThought I\\'d end up with Sean\\nBu...   \n",
      "1      [\"[Verse 1]\\nFound you when your heart was bro...   \n",
      "2      ['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...   \n",
      "4      [\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...   \n",
      "5      [\"[Intro]\\nI-I-I don't want a lot for Christma...   \n",
      "...                                                  ...   \n",
      "20399  ['[Verse 1: Big Boi]\\nWell, it\\'s the M-I-croo...   \n",
      "20400  ['[Intro]\\nThere are times when I look above a...   \n",
      "20401  [\"[Intro: Prodigy and Havoc]\\nWord up son, wor...   \n",
      "20402  [\"[Chorus]\\nWee-ooh wim-o-weh. Wee-ooh wim-o-w...   \n",
      "20403  ['[Intro: Shaq]\\nYo Jef, why don\\'t you give m...   \n",
      "\n",
      "                                             single_text  \n",
      "0      thank you next next \\n thank you next next \\n ...  \n",
      "1      tell me hows it feel sittin up there \\n feelin...  \n",
      "2      woo made this here with all the ice on in the ...  \n",
      "4      had to have high high hopes for a living \\n sh...  \n",
      "5      i dont want a lot for christmas \\n there is ju...  \n",
      "...                                                  ...  \n",
      "20399  my heat is in the trunk along with that quad k...  \n",
      "20400  there are times when i look above and beyond \\...  \n",
      "20401  i got you stuck off the realness we be the inf...  \n",
      "20402  in the jungle the mighty jungle the lion sleep...  \n",
      "20403  you wanna fight come fight me \\n ill hit ya wi...  \n",
      "\n",
      "[19663 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# load lyrics data from MusicOSet \n",
    "# should show a dataframe with 20000 song_ids and their lyrics\n",
    "df = pd.read_csv(\"musicoset_songfeatures/lyrics.csv\", sep=\"\\t\")\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# adding poems from The Poetry Foundation (14000 poems, author and tags assoiated with poem)\n",
    "# we combined both sources to increase data quali5y\n",
    "pdf = pd.read_csv('musicoset_songfeatures/PoetryFoundationData.csv',quotechar='\"')\n",
    "pdf.head()\n",
    "df = df.dropna()\n",
    "\n",
    "# initializing string stranslator to clean punctutation before training\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# splits lyrics into intro, verses, and chorus, only selects first 4 verses + chorus\n",
    "def split_text(x):\n",
    "   text = x['lyrics']\n",
    "   sections = text.split('\\\\n\\\\n')\n",
    "   keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n",
    "   lyrics = str()\n",
    "   single_text = []\n",
    "   res = {}\n",
    "   for s in sections:\n",
    "       key = s[s.find('[') + 1:s.find(']')].strip()\n",
    "       if ':' in key:\n",
    "           key = key[:key.find(':')]\n",
    "          \n",
    "       if key in keys:\n",
    "           single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n",
    "       res['single_text'] =  ' \\n '.join(single_text)\n",
    "   return pd.Series(res)\n",
    "# joins resulting text into a single text\n",
    "df = df.join(df.apply(split_text, axis=1))\n",
    "df.head()\n",
    "\n",
    "print(df)\n",
    "\n",
    "# # Testing\n",
    "# lines = ''\n",
    "# for i in df.head(1)['lyrics']:\n",
    "#     lines = i.split('\\\\n\\\\n')\n",
    "# print (lines)\n",
    "# df['lyrics'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a495972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning poems text\n",
    "pdf['single_text'] = pdf['Poem'].apply(lambda x: ' \\n '.join([l.lower().strip().translate(translator) for l in x.splitlines() if len(l)>0]))\n",
    "pdf.head()\n",
    "\n",
    "# combine poems dataframe and lyrics dataframe\n",
    "sum_df = pd.DataFrame( df['single_text'] )\n",
    "sum_df = pd.concat([df, pd.DataFrame( pdf['single_text'])])\n",
    "sum_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16d7f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  2339637\n",
      "Words with less than 7 appearances: 26272\n",
      "Words with more than 7 appearances: 9163\n",
      "Valid sequences of size 5: 2087702\n"
     ]
    }
   ],
   "source": [
    "text_as_list = []\n",
    "frequencies = {}\n",
    "uncommon_words = set()\n",
    "MIN_FREQUENCY = 7\n",
    "MIN_SEQ = 5\n",
    "BATCH_SIZE =  32\n",
    "\n",
    "def extract_text(text):\n",
    "   global text_as_list\n",
    "   text_as_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "   \n",
    "df['single_text'].apply(extract_text)\n",
    "print('Total words: ', len(text_as_list))\n",
    "for w in text_as_list:\n",
    "   frequencies[w] = frequencies.get(w, 0) + 1\n",
    "  \n",
    "uncommon_words = set([key for key in frequencies.keys() if frequencies[key] < MIN_FREQUENCY])\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= MIN_FREQUENCY]))\n",
    "num_words = len(words)\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n",
    "print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n",
    "valid_seqs = []\n",
    "end_seq_words = []\n",
    "for i in range(len(text_as_list) - MIN_SEQ ):\n",
    "   end_slice = i + MIN_SEQ + 1\n",
    "   if len( set(text_as_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n",
    "       valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
    "       end_seq_words.append(text_as_list[i + MIN_SEQ])\n",
    "      \n",
    "print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc186504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in pretrained tokenizer and model. Using GPT2\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "model = GPT2Model.from_pretrained(\"openai-community/gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68a48abd-0ee9-4f3e-981a-07313fc125da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', 'w') as f:\n",
    "  for t in X_train:\n",
    "    t = ' '.join(t)\n",
    "    f.write(t)\n",
    "    f.write(' ')\n",
    "\n",
    "\n",
    "with open('test.txt', 'w') as f:\n",
    "  for t in X_test:\n",
    "    t = ' '.join(t)\n",
    "    f.write(t)\n",
    "    f.write(' ')\n",
    "\n",
    "train_path = 'train.txt'\n",
    "test_path = 'test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28503924-66fb-4902-bc8e-40b1b862a41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6f76406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14476"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "           tokenizer=tokenizer,\n",
    "           file_path=train_path,\n",
    "           block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "           tokenizer=tokenizer,\n",
    "           file_path=test_path,\n",
    "           block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "         tokenizer=tokenizer, mlm=False,\n",
    "     )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)\n",
    "\n",
    "len(train_dataset)\n",
    "len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "250656b8-b60c-4acb-93ab-e5edbceb3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, log_loss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from transformers import TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "259dae51",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 33\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import torch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# lang = 'en'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# #         'loss@'+lang: loss,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# #     }\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.11/site-packages/transformers/trainer.py:3091\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstop_and_update_metrics(output\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m   3087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PredictionOutput(predictions\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mpredictions, label_ids\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mlabel_ids, metrics\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m   3089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluation_loop\u001b[39m(\n\u001b[1;32m   3090\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m-> 3091\u001b[0m     dataloader: DataLoader,\n\u001b[1;32m   3092\u001b[0m     description: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   3093\u001b[0m     prediction_loss_only: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3094\u001b[0m     ignore_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3095\u001b[0m     metric_key_prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EvalLoopOutput:\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m    Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\u001b[39;00m\n\u001b[1;32m   3099\u001b[0m \n\u001b[1;32m   3100\u001b[0m \u001b[38;5;124;03m    Works both with or without labels.\u001b[39;00m\n\u001b[1;32m   3101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3102\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.11/site-packages/transformers/trainer.py:867\u001b[0m, in \u001b[0;36mget_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.11/site-packages/accelerate/accelerator.py:1230\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1220\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m   1221\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(obj)\n\u001b[1;32m   1222\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mNO\n\u001b[1;32m   1223\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_BYPASS_DEVICE_MAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1224\u001b[0m     ):\n\u001b[1;32m   1225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1226\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded with `device_map=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` in any distributed mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mmyscript.py}}`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1228\u001b[0m         )\n\u001b[0;32m-> 1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1231\u001b[0m     model_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m args:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.11/site-packages/accelerate/accelerator.py:515\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/new-env/lib/python3.11/site-packages/accelerate/state.py:1077\u001b[0m, in \u001b[0;36mAcceleratorState.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# By this point we know that no attributes of `self` contain `name`,\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# so we just modify the error message\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_attrs:\n\u001b[0;32m-> 1077\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1078\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`AcceleratorState` object has no attribute `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1079\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis happens if `AcceleratorState._reset_state()` was called and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1080\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man `Accelerator` or `PartialState` was not reinitialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1081\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# Raise a typical AttributeError\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcceleratorState\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# lang = 'en'\n",
    "\n",
    "# class CustomCallback(TrainerCallback):\n",
    "#     def __init__(self, trainer) -> None:\n",
    "#         super().__init__()\n",
    "#         self._trainer = trainer\n",
    "    \n",
    "#     def on_epoch_end(self, args, state, control, **kwargs):\n",
    "#         if control.should_evaluate:\n",
    "#             control_copy = deepcopy(control)\n",
    "#             self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train@\"+lang)\n",
    "#             return control_copy\n",
    "\n",
    "# # def compute_metrics(pred):\n",
    "# #     global num_labels\n",
    "# #     labels = pred.label_ids\n",
    "# #     preds = pred.predictions.argmax(-1)\n",
    "# #     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "# #     acc = accuracy_score(labels, preds)\n",
    "# #     loss_fct = CrossEntropyLoss()\n",
    "# #     logits = torch.tensor(pred.predictions)\n",
    "# #     labels = torch.tensor(labels)\n",
    "# #     loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "# #     return {\n",
    "# #         'accuracy@'+lang: acc,\n",
    "# #         'f1@'+lang: f1,\n",
    "# #         'precision@'+lang: precision,\n",
    "# #         'recall@'+lang: recall,\n",
    "# #         'loss@'+lang: loss,\n",
    "# #     }\n",
    "\n",
    "results = trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

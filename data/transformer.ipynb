{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb0087e6-3cf4-4185-ae00-a772bc5311a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20404 entries, 0 to 20403\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   song_id  20404 non-null  object\n",
      " 1   lyrics   19663 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 318.9+ KB\n",
      "                      song_id  \\\n",
      "0      3e9HZxeyfWwjeyPAMmWSSQ   \n",
      "1      5p7ujcrUXASCNwRaWNHR1C   \n",
      "2      2xLMifQCjDGFmkHkpNLD9h   \n",
      "4      1rqqCSm0Qe4I9rUvWncaom   \n",
      "5      0bYg9bo50gSsH3LtXe2SQn   \n",
      "...                       ...   \n",
      "20399  2pMAmZdHfQHyqJCXJbfhK3   \n",
      "20400  0IaMMHVbpJ0LrRAeigWOXr   \n",
      "20401  4nASzyRbzL5qZQuOPjQfsj   \n",
      "20402  2F4FNcz68howQWD4zaGJSi   \n",
      "20403  0TEQ2QmFXnHCgQvYuvsbp2   \n",
      "\n",
      "                                                  lyrics  \\\n",
      "0      ['[Verse 1]\\nThought I\\'d end up with Sean\\nBu...   \n",
      "1      [\"[Verse 1]\\nFound you when your heart was bro...   \n",
      "2      ['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...   \n",
      "4      [\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...   \n",
      "5      [\"[Intro]\\nI-I-I don't want a lot for Christma...   \n",
      "...                                                  ...   \n",
      "20399  ['[Verse 1: Big Boi]\\nWell, it\\'s the M-I-croo...   \n",
      "20400  ['[Intro]\\nThere are times when I look above a...   \n",
      "20401  [\"[Intro: Prodigy and Havoc]\\nWord up son, wor...   \n",
      "20402  [\"[Chorus]\\nWee-ooh wim-o-weh. Wee-ooh wim-o-w...   \n",
      "20403  ['[Intro: Shaq]\\nYo Jef, why don\\'t you give m...   \n",
      "\n",
      "                                             single_text  \n",
      "0      thank you next next \\n thank you next next \\n ...  \n",
      "1      tell me hows it feel sittin up there \\n feelin...  \n",
      "2      woo made this here with all the ice on in the ...  \n",
      "4      had to have high high hopes for a living \\n sh...  \n",
      "5      i dont want a lot for christmas \\n there is ju...  \n",
      "...                                                  ...  \n",
      "20399  my heat is in the trunk along with that quad k...  \n",
      "20400  there are times when i look above and beyond \\...  \n",
      "20401  i got you stuck off the realness we be the inf...  \n",
      "20402  in the jungle the mighty jungle the lion sleep...  \n",
      "20403  you wanna fight come fight me \\n ill hit ya wi...  \n",
      "\n",
      "[19663 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load lyrics data from MusicOSet \n",
    "# should show a dataframe with 20000 song_ids and their lyrics\n",
    "df = pd.read_csv(\"musicoset_songfeatures/lyrics.csv\", sep=\"\\t\")\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# adding poems from The Poetry Foundation (14000 poems, author and tags assoiated with poem)\n",
    "# we combined both sources to increase data quali5y\n",
    "pdf = pd.read_csv('musicoset_songfeatures/PoetryFoundationData.csv',quotechar='\"')\n",
    "pdf.head()\n",
    "df = df.dropna()\n",
    "\n",
    "# initializing string stranslator to clean punctutation before training\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# splits lyrics into intro, verses, and chorus, only selects first 4 verses + chorus\n",
    "def split_text(x):\n",
    "   text = x['lyrics']\n",
    "   sections = text.split('\\\\n\\\\n')\n",
    "   keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n",
    "   lyrics = str()\n",
    "   single_text = []\n",
    "   res = {}\n",
    "   for s in sections:\n",
    "       key = s[s.find('[') + 1:s.find(']')].strip()\n",
    "       if ':' in key:\n",
    "           key = key[:key.find(':')]\n",
    "          \n",
    "       if key in keys:\n",
    "           single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n",
    "       res['single_text'] =  ' \\n '.join(single_text)\n",
    "   return pd.Series(res)\n",
    "# joins resulting text into a single text\n",
    "df = df.join(df.apply(split_text, axis=1))\n",
    "df.head()\n",
    "\n",
    "print(df)\n",
    "\n",
    "# # Testing\n",
    "# lines = ''\n",
    "# for i in df.head(1)['lyrics']:\n",
    "#     lines = i.split('\\\\n\\\\n')\n",
    "# print (lines)\n",
    "# df['lyrics'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a495972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning poems text\n",
    "pdf['single_text'] = pdf['Poem'].apply(lambda x: ' \\n '.join([l.lower().strip().translate(translator) for l in x.splitlines() if len(l)>0]))\n",
    "pdf.head()\n",
    "\n",
    "# combine poems dataframe and lyrics dataframe\n",
    "sum_df = pd.DataFrame( df['single_text'] )\n",
    "sum_df = pd.concat([df, pd.DataFrame( pdf['single_text'])])\n",
    "sum_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16d7f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  2339637\n",
      "Words with less than 7 appearances: 26272\n",
      "Words with more than 7 appearances: 9163\n",
      "Valid sequences of size 5: 2087702\n"
     ]
    }
   ],
   "source": [
    "text_as_list = []\n",
    "frequencies = {}\n",
    "uncommon_words = set()\n",
    "MIN_FREQUENCY = 7\n",
    "MIN_SEQ = 5\n",
    "BATCH_SIZE =  32\n",
    "\n",
    "def extract_text(text):\n",
    "   global text_as_list\n",
    "   text_as_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "   \n",
    "df['single_text'].apply(extract_text)\n",
    "print('Total words: ', len(text_as_list))\n",
    "for w in text_as_list:\n",
    "   frequencies[w] = frequencies.get(w, 0) + 1\n",
    "  \n",
    "uncommon_words = set([key for key in frequencies.keys() if frequencies[key] < MIN_FREQUENCY])\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= MIN_FREQUENCY]))\n",
    "num_words = len(words)\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n",
    "print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n",
    "valid_seqs = []\n",
    "end_seq_words = []\n",
    "for i in range(len(text_as_list) - MIN_SEQ ):\n",
    "   end_slice = i + MIN_SEQ + 1\n",
    "   if len( set(text_as_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n",
    "       valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
    "       end_seq_words.append(text_as_list[i + MIN_SEQ])\n",
    "      \n",
    "print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc186504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in pretrained tokenizer and model. Using GPT2\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "model = GPT2Model.from_pretrained(\"openai-community/gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68a48abd-0ee9-4f3e-981a-07313fc125da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', 'w') as f:\n",
    "  for t in X_train:\n",
    "    t = ' '.join(t)\n",
    "    f.write(t)\n",
    "    f.write(' ')\n",
    "\n",
    "\n",
    "with open('test.txt', 'w') as f:\n",
    "  for t in X_test:\n",
    "    t = ' '.join(t)\n",
    "    f.write(t)\n",
    "    f.write(' ')\n",
    "\n",
    "train_path = 'train.txt'\n",
    "test_path = 'test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28503924-66fb-4902-bc8e-40b1b862a41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flow like dead people i'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6f76406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malki\\anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "           tokenizer=tokenizer,\n",
    "           file_path=train_path,\n",
    "           block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "           tokenizer=tokenizer,\n",
    "           file_path=test_path,\n",
    "           block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "         tokenizer=tokenizer, mlm=False,\n",
    "     )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "250656b8-b60c-4acb-93ab-e5edbceb3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e4be4ea-a7b4-4e9b-9baf-f0776fabcf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malki\\anaconda3\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt-2\", \n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=300, \n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_steps = 100, \n",
    "    save_steps=800, \n",
    "    warmup_steps=500\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9bc9454-22a7-4a48-82fe-2d4fc7a67c3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1807\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[1;32m-> 1809\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[0;32m   1811\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:853\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    844\u001b[0m dataloader_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size,\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_collator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersistent_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_persistent_workers,\n\u001b[0;32m    850\u001b[0m }\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterableDataset):\n\u001b[1;32m--> 853\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_train_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[0;32m    855\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:823\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LengthGroupedSampler(\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps,\n\u001b[0;32m    817\u001b[0m         dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset,\n\u001b[0;32m    818\u001b[0m         lengths\u001b[38;5;241m=\u001b[39mlengths,\n\u001b[0;32m    819\u001b[0m         model_input_name\u001b[38;5;241m=\u001b[39mmodel_input_name,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd82adef-b039-4a31-a9b4-128538e70273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8868f259-731d-410b-9439-5539c754cadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.data.datasets.language_modeling.TextDataset at 0x26d9f9f1100>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128536de-27d0-468a-90af-a4cd2dcf4acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

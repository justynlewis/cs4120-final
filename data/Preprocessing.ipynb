{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0087e6-3cf4-4185-ae00-a772bc5311a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense,  Activation\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback, EarlyStopping\n",
    "\n",
    "\n",
    "# load lyrics data from MusicOSet \n",
    "# should show a dataframe with 20000 song_ids and their lyrics\n",
    "df = pd.read_csv(\"/Users/jlsan/Documents/GitHub/cs4120-final/musicoset_songfeatures/lyrics.csv\", sep=\"\\t\")\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# adding poems from The Poetry Foundation (14000 poems, author and tags assoiated with poem)\n",
    "# we combined both sources to increase data quali5y\n",
    "pdf = pd.read_csv('/Users/jlsan/Documents/GitHub/cs4120-final/musicoset_songfeatures/PoetryFoundationData.csv',quotechar='\"')\n",
    "pdf.head()\n",
    "df = df.dropna()\n",
    "\n",
    "# initializing string stranslator to clean punctutation before training\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# splits lyrics into intro, verses, and chorus, only selects first 4 verses + chorus\n",
    "def split_text(x):\n",
    "   text = x['lyrics']\n",
    "   sections = text.split('\\\\n\\\\n')\n",
    "   keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n",
    "   lyrics = str()\n",
    "   single_text = []\n",
    "   res = {}\n",
    "   for s in sections:\n",
    "       key = s[s.find('[') + 1:s.find(']')].strip()\n",
    "       if ':' in key:\n",
    "           key = key[:key.find(':')]\n",
    "          \n",
    "       if key in keys:\n",
    "           single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n",
    "       res['single_text'] =  ' \\n '.join(single_text)\n",
    "   return pd.Series(res)\n",
    "# joins resulting text into a single text\n",
    "df = df.join(df.apply(split_text, axis=1))\n",
    "df.head()\n",
    "\n",
    "print(df)\n",
    "\n",
    "# # Testing\n",
    "# lines = ''\n",
    "# for i in df.head(1)['lyrics']:\n",
    "#     lines = i.split('\\\\n\\\\n')\n",
    "# print (lines)\n",
    "# df['lyrics'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning poems text\n",
    "pdf['single_text'] = pdf['Poem'].apply(lambda x: ' \\n '.join([l.lower().strip().translate(translator) for l in x.splitlines() if len(l)>0]))\n",
    "pdf.head()\n",
    "\n",
    "# combine poems dataframe and lyrics dataframe\n",
    "sum_df = pd.DataFrame( df['single_text'] )\n",
    "sum_df = pd.concat([df, pd.DataFrame( pdf['single_text'])])\n",
    "sum_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_list = []  # List to store all words extracted from the text\n",
    "frequencies = {}   # Dictionary for word frequencies\n",
    "uncommon_words = set()  # Set for uncommon words\n",
    "MIN_FREQUENCY = 7  # Minimum frequency threshold for words\n",
    "MIN_SEQ = 5        # Minimum sequence length\n",
    "BATCH_SIZE = 32    # Batch size for data generator\n",
    "\n",
    "def extract_text(text):\n",
    "    global text_as_list\n",
    "    text_as_list += [word for word in text.split(' ') if word.strip() != '' or word == '\\n']\n",
    "\n",
    "df['single_text'].apply(extract_text)\n",
    "print('Total words:', len(text_as_list))\n",
    "\n",
    "for word in text_as_list:\n",
    "    frequencies[word] = frequencies.get(word, 0) + 1\n",
    "\n",
    "uncommon_words = {key for key in frequencies.keys() if frequencies[key] < MIN_FREQUENCY}\n",
    "words = sorted({key for key in frequencies.keys() if frequencies[key] >= MIN_FREQUENCY})\n",
    "num_words = len(words)\n",
    "word_indices = {word: i for i, word in enumerate(words)}\n",
    "indices_word = {i: word for i, word in enumerate(words)}\n",
    "print('Words with less than {} appearances: {}'.format(MIN_FREQUENCY, len(uncommon_words)))\n",
    "print('Words with more than {} appearances: {}'.format(MIN_FREQUENCY, len(words)))\n",
    "\n",
    "valid_seqs = []   \n",
    "end_seq_words = []  \n",
    "\n",
    "for i in range(len(text_as_list) - MIN_SEQ):\n",
    "    end_slice = i + MIN_SEQ + 1\n",
    "    if len(set(text_as_list[i:end_slice]).intersection(uncommon_words)) == 0:\n",
    "        valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
    "        end_seq_words.append(text_as_list[i + MIN_SEQ])\n",
    "\n",
    "print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.02, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc98360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for fit and evaluate\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, MIN_SEQ), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word_indices[w]\n",
    "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "\n",
    "# Function to sample the next word\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function to generate text at the end of each epoch\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at the end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "    \n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(X_train + X_test))\n",
    "    seed = (X_train + X_test)[seed_index]\n",
    " \n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, MIN_SEQ))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t] = word_indices[word]\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    " \n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    " \n",
    "            examples_file.write(\" \" + next_word)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('=' * 80 + '\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines and builds the model architechture\n",
    "def get_model():\n",
    "   print('Building model...')\n",
    "   model = Sequential()\n",
    "   model.add(Embedding(input_dim=num_words, output_dim=1024))\n",
    "   model.add(Bidirectional(LSTM(128)))\n",
    "   model.add(Dense(num_words))\n",
    "   model.add(Activation('softmax'))\n",
    "   return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81921fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = get_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# Define the file path for saving model checkpoints\n",
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
    "           \"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}.keras\" % \\\n",
    "           (num_words, MIN_SEQ, MIN_FREQUENCY)\n",
    "\n",
    "# Configure model checkpoint to save the best model\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Define a callback to print generated text at the end of each epoch\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "\n",
    "# List of callbacks\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "# Open a file to store generated examples\n",
    "examples_file = open('examples.txt', \"w\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(generator(X_train, y_train, BATCH_SIZE),\n",
    "                   steps_per_epoch=int(len(valid_seqs)/BATCH_SIZE) + 1,\n",
    "                   epochs=10,\n",
    "                   callbacks=callbacks_list,\n",
    "                   validation_data=generator(X_test, y_train, BATCH_SIZE),\n",
    "                   validation_steps=int(len(y_train)/BATCH_SIZE) + 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

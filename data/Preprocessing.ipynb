{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0087e6-3cf4-4185-ae00-a772bc5311a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20404 entries, 0 to 20403\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   song_id  20404 non-null  object\n",
      " 1   lyrics   19663 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 318.9+ KB\n",
      "                      song_id  \\\n",
      "0      3e9HZxeyfWwjeyPAMmWSSQ   \n",
      "1      5p7ujcrUXASCNwRaWNHR1C   \n",
      "2      2xLMifQCjDGFmkHkpNLD9h   \n",
      "4      1rqqCSm0Qe4I9rUvWncaom   \n",
      "5      0bYg9bo50gSsH3LtXe2SQn   \n",
      "...                       ...   \n",
      "20399  2pMAmZdHfQHyqJCXJbfhK3   \n",
      "20400  0IaMMHVbpJ0LrRAeigWOXr   \n",
      "20401  4nASzyRbzL5qZQuOPjQfsj   \n",
      "20402  2F4FNcz68howQWD4zaGJSi   \n",
      "20403  0TEQ2QmFXnHCgQvYuvsbp2   \n",
      "\n",
      "                                                  lyrics  \\\n",
      "0      ['[Verse 1]\\nThought I\\'d end up with Sean\\nBu...   \n",
      "1      [\"[Verse 1]\\nFound you when your heart was bro...   \n",
      "2      ['[Part I]\\n\\n[Intro: Drake]\\nAstro, yeah\\nSun...   \n",
      "4      [\"[Intro]\\nHigh, high hopes\\n\\n[Chorus]\\nHad t...   \n",
      "5      [\"[Intro]\\nI-I-I don't want a lot for Christma...   \n",
      "...                                                  ...   \n",
      "20399  ['[Verse 1: Big Boi]\\nWell, it\\'s the M-I-croo...   \n",
      "20400  ['[Intro]\\nThere are times when I look above a...   \n",
      "20401  [\"[Intro: Prodigy and Havoc]\\nWord up son, wor...   \n",
      "20402  [\"[Chorus]\\nWee-ooh wim-o-weh. Wee-ooh wim-o-w...   \n",
      "20403  ['[Intro: Shaq]\\nYo Jef, why don\\'t you give m...   \n",
      "\n",
      "                                             single_text  \n",
      "0      thank you next next \\n thank you next next \\n ...  \n",
      "1      tell me hows it feel sittin up there \\n feelin...  \n",
      "2      woo made this here with all the ice on in the ...  \n",
      "4      had to have high high hopes for a living \\n sh...  \n",
      "5      i dont want a lot for christmas \\n there is ju...  \n",
      "...                                                  ...  \n",
      "20399  my heat is in the trunk along with that quad k...  \n",
      "20400  there are times when i look above and beyond \\...  \n",
      "20401  i got you stuck off the realness we be the inf...  \n",
      "20402  in the jungle the mighty jungle the lion sleep...  \n",
      "20403  you wanna fight come fight me \\n ill hit ya wi...  \n",
      "\n",
      "[19663 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load lyrics data from MusicOSet \n",
    "# should show a dataframe with 20000 song_ids and their lyrics\n",
    "df = pd.read_csv(\"musicoset_songfeatures/lyrics.csv\", sep=\"\\t\")\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# adding poems from The Poetry Foundation (14000 poems, author and tags assoiated with poem)\n",
    "# we combined both sources to increase data quali5y\n",
    "pdf = pd.read_csv('musicoset_songfeatures/PoetryFoundationData.csv',quotechar='\"')\n",
    "pdf.head()\n",
    "df = df.dropna()\n",
    "\n",
    "# initializing string stranslator to clean punctutation before training\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# splits lyrics into intro, verses, and chorus, only selects first 4 verses + chorus\n",
    "def split_text(x):\n",
    "   text = x['lyrics']\n",
    "   sections = text.split('\\\\n\\\\n')\n",
    "   keys = {'Verse 1': np.nan,'Verse 2':np.nan,'Verse 3':np.nan,'Verse 4':np.nan, 'Chorus':np.nan}\n",
    "   lyrics = str()\n",
    "   single_text = []\n",
    "   res = {}\n",
    "   for s in sections:\n",
    "       key = s[s.find('[') + 1:s.find(']')].strip()\n",
    "       if ':' in key:\n",
    "           key = key[:key.find(':')]\n",
    "          \n",
    "       if key in keys:\n",
    "           single_text += [x.lower().replace('(','').replace(')','').translate(translator) for x in s[s.find(']')+1:].split('\\\\n') if len(x) > 1]\n",
    "       res['single_text'] =  ' \\n '.join(single_text)\n",
    "   return pd.Series(res)\n",
    "# joins resulting text into a single text\n",
    "df = df.join(df.apply(split_text, axis=1))\n",
    "df.head()\n",
    "\n",
    "print(df)\n",
    "\n",
    "# # Testing\n",
    "# lines = ''\n",
    "# for i in df.head(1)['lyrics']:\n",
    "#     lines = i.split('\\\\n\\\\n')\n",
    "# print (lines)\n",
    "# df['lyrics'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a495972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning poems text\n",
    "pdf['single_text'] = pdf['Poem'].apply(lambda x: ' \\n '.join([l.lower().strip().translate(translator) for l in x.splitlines() if len(l)>0]))\n",
    "pdf.head()\n",
    "\n",
    "# combine poems dataframe and lyrics dataframe\n",
    "sum_df = pd.DataFrame( df['single_text'] )\n",
    "sum_df = pd.concat([df, pd.DataFrame( pdf['single_text'])])\n",
    "sum_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d7f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  2339637\n",
      "Words with less than 7 appearances: 26272\n",
      "Words with more than 7 appearances: 9163\n",
      "Valid sequences of size 5: 2087702\n"
     ]
    }
   ],
   "source": [
    "text_as_list = []\n",
    "frequencies = {}\n",
    "uncommon_words = set()\n",
    "MIN_FREQUENCY = 7\n",
    "MIN_SEQ = 5\n",
    "BATCH_SIZE =  32\n",
    "\n",
    "def extract_text(text):\n",
    "   global text_as_list\n",
    "   text_as_list += [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "   \n",
    "df['single_text'].apply(extract_text)\n",
    "print('Total words: ', len(text_as_list))\n",
    "for w in text_as_list:\n",
    "   frequencies[w] = frequencies.get(w, 0) + 1\n",
    "  \n",
    "uncommon_words = set([key for key in frequencies.keys() if frequencies[key] < MIN_FREQUENCY])\n",
    "words = sorted(set([key for key in frequencies.keys() if frequencies[key] >= MIN_FREQUENCY]))\n",
    "num_words = len(words)\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))\n",
    "print('Words with less than {} appearances: {}'.format( MIN_FREQUENCY, len(uncommon_words)))\n",
    "print('Words with more than {} appearances: {}'.format( MIN_FREQUENCY, len(words)))\n",
    "valid_seqs = []\n",
    "end_seq_words = []\n",
    "for i in range(len(text_as_list) - MIN_SEQ ):\n",
    "   end_slice = i + MIN_SEQ + 1\n",
    "   if len( set(text_as_list[i:end_slice]).intersection(uncommon_words) ) == 0:\n",
    "       valid_seqs.append(text_as_list[i: i + MIN_SEQ])\n",
    "       end_seq_words.append(text_as_list[i + MIN_SEQ])\n",
    "      \n",
    "print('Valid sequences of size {}: {}'.format(MIN_SEQ, len(valid_seqs)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(valid_seqs, end_seq_words, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc186504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justynlewis/opt/anaconda3/envs/new-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# load in pretrained tokenizer and model. Using GPT2\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "model = GPT2Model.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# with open('traincancionesDiomedes.txt', 'w') as f:\n",
    "#   for t in X_train:\n",
    "#     f.write(t)\n",
    "#     f.write(' ')\n",
    "\n",
    "\n",
    "# with open('testcancionesDiomedes.txt', 'w') as f:\n",
    "#   for t in X_test:\n",
    "#     f.write(t)\n",
    "#     f.write(' ')\n",
    "\n",
    "train_path = 'train.txt'\n",
    "test_path = 'test.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f76406",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "# def load_dataset(train_path,test_path,tokenizer):\n",
    "#     train_dataset = TextDataset(\n",
    "#           tokenizer=tokenizer,\n",
    "#           file_path=train_path,\n",
    "#           block_size=128)\n",
    "\n",
    "#     test_dataset = TextDataset(\n",
    "#           tokenizer=tokenizer,\n",
    "#           file_path=test_path,\n",
    "#           block_size=128)\n",
    "\n",
    "#     data_collator = DataCollatorForLanguageModeling(\n",
    "#         tokenizer=tokenizer, mlm=False,\n",
    "#     )\n",
    "#     return train_dataset,test_dataset,data_collator\n",
    "\n",
    "# train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
